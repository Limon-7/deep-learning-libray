{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Saving a model: We can save a modle using pytorch `torch.save` method. We can also save the ***leraned parameter*** in an internal state dictionary called `state_dict()`. ***`Syntax: torch.save(model.state_dict(), \"data/model.pth\")`.***\n",
    "2. load the model: To load the model, we will define the model class which contains the state and parameters of the neural network used to train the model.When loading model weights, we needed to instantiate the model class first, because the class defines the structure of a network. Next, we load the parameters using the `load_state_dict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessay modules\n",
    "import torch\n",
    "import torchinfo\n",
    "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "from torch.utils.data import Dataset, DataLoader # Gives easier dataset managment by creating mini batches etc.\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
    "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
    "from torch import nn  # All neural network modules\n",
    "from tqdm import tqdm # for nice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device config\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CNN layer:\n",
    "class CNNNet(nn.Module):\n",
    "    def __init__(self, in_channels =1, num_classes=10):\n",
    "        super(CNNNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels= 8, kernel_size=(3,3),stride=(1,1), padding=(1,1))\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(in_channels = 8, out_channels= 16, kernel_size=(3,3),stride=(1,1), padding=(1,1))\n",
    "        self.fc1 = nn.Linear(16*7*7, num_classes)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "in_channel = 1 # for minist 28*28\n",
    "num_classes = 10 # 0-9\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 3\n",
    "load_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "CNNNet                                   --\n",
      "├─Conv2d: 1-1                            80\n",
      "├─MaxPool2d: 1-2                         --\n",
      "├─Conv2d: 1-3                            1,168\n",
      "├─Linear: 1-4                            7,850\n",
      "=================================================================\n",
      "Total params: 9,098\n",
      "Trainable params: 9,098\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize network\n",
    "model = CNNNet().to(device)\n",
    "print(torchinfo.summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, file_name=\"checkpoints/my_checkpoint.pth.tar\"):\n",
    "    print(\"=>saving the checkpoint\")\n",
    "    torch.save(state, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, checkpoint):\n",
    "    print(\"=> loading checkpoint:: \")\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint:: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/938 [00:00<02:36,  5.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 43.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at each epoch 0.02685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 49.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at each epoch 0.02519\n",
      "=>saving the checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 49.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at each epoch 0.02254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Network\n",
    "def train_network(num_epochs, loader, model, device, loss_fn, optimizer):\n",
    "    if load_model:\n",
    "        load_checkpoint(model, torch.load(\"checkpoints/my_checkpoint.pth.tar\"))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        check_point = {'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "        if(epoch==2):\n",
    "            save_checkpoint(check_point)\n",
    "        \n",
    "        for batch_idx, (data, targets) in enumerate(tqdm(loader)):\n",
    "            # Get data to cuda if possible\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            # Forward\n",
    "            scores = model(data)\n",
    "            loss = loss_fn(scores, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient descent or adam step\n",
    "            optimizer.step()\n",
    "            \n",
    "        mean_loss = sum(losses)/len(losses)\n",
    "        print(f\"loss at each epoch {mean_loss:.5f}\")\n",
    "            \n",
    "train_network(num_epochs, train_loader, model, device, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint:: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 43.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at each epoch 0.02301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 49.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at each epoch 0.02069\n",
      "=>saving the checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:19<00:00, 49.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at each epoch 0.01947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_network(num_epochs, train_loader, model, device, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    # We don't need to keep track of gradients here so we wrap it in torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        # Loop through the data\n",
    "        for x, y in loader:\n",
    "\n",
    "            # Move data to device\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "\n",
    "            # Check how many we got correct\n",
    "            num_correct += (predictions == y).sum()\n",
    "\n",
    "            # Keep track of number of samples\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    model.train()\n",
    "    return num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 98.05\n",
      "Accuracy on test set: 98.00\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Mistakee:\n",
    "1. **Didn't overfit batch**: Overfit a ***single batch to check NN*** is working perfectly without having any errors. Then passing the whole dataset. `data, labels = next(iter(train_loader))`\n",
    "\n",
    "2. ***Forgot toggle tain/eval:*** do first `model.eval()` then `test with  with torch.no_grad():` and finally train again by `model.train()`. This will improve model performance.\n",
    "\n",
    "3. **forgot.zero_grad():** `optimizer.zero_grad()` is used to clear the gradients of all model parameters before the next iteration.\n",
    "\n",
    "4. **Softmax when using CrossEntropy:** If we use `CrossEntropyLoss()` this will internally add a `sofmax` function.\n",
    "\n",
    "5. ***Bias term with BatchNorm:*** When we add batchNorm after a **CNN layer or Linear layer** we have set `bais=False` for those layer. Because BatchNorm layer add the bias term internally.\n",
    "\n",
    "6. **Using View as Permute:**\n",
    "    * `view:` The view function is used to reshape a tensor while maintaining the total number of elements.\n",
    "    * `permute:` The permute function is used to rearrange the dimensions of a tensor.\n",
    "\n",
    "7. **Incorrect Data Augmentation:**\n",
    "\n",
    "8. **Not shuffling data:**\n",
    "\n",
    "9. **Not normalize Data:** Add a Normalize in the transform.\n",
    "\n",
    "10. **Not cliping Gradient:(`torch.nn.utils.clip_grad_norm(model.parameters(), max_norm=1)`)** It is commonly used during training to clip gradients to a specified maximum norm in order to prevent exploding gradients and improve the stability of the optimization process. It is commonly used in `RNNs, LSTMs, GRUs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import Section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessay modules\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "from torch.utils.data import Dataset, DataLoader # Gives easier dataset managment by creating mini batches etc.\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
    "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
    "from torch import nn  # All neural network modules\n",
    "from tqdm import tqdm # for nice \n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Config Section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device config\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "input_size = 784 # for minist 28*28\n",
    "num_classes = 10 # 0-9\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Data Processing Section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, transform=transforms.ToTensor(), download=True\n",
    ")\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x000001D120946490>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Model Section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NN                                       [1, 1, 10]                --\n",
       "├─Linear: 1-1                            [1, 1, 50]                39,250\n",
       "├─Dropout: 1-2                           [1, 1, 50]                --\n",
       "├─Linear: 1-3                            [1, 1, 10]                510\n",
       "==========================================================================================\n",
       "Total params: 39,760\n",
       "Trainable params: 39,760\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.04\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.16\n",
       "Estimated Total Size (MB): 0.16\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize network\n",
    "model = NN(input_size=input_size, num_classes=num_classes).to(device)\n",
    "summary(model, input_size=(1,1,28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Loss and Optimizer Section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Train Model Section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:12<00:00, 76.70it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at each epoch 0.66677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:09<00:00, 98.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at each epoch 0.40861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:09<00:00, 99.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at each epoch 0.35974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Network\n",
    "def train_nn(model, num_epochs, loader, loss_fn, optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        losses=[]\n",
    "        for batch_idx, (data, targets) in enumerate(tqdm(loader)):\n",
    "            # Get data to cuda if possible\n",
    "            data = data.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            data = data.reshape(data.shape[0], -1)\n",
    "            # Forward\n",
    "            scores = model(data)\n",
    "            loss = loss_fn(scores, targets)\n",
    "            losses.append(loss)\n",
    "\n",
    "            # Backward\n",
    "            optimizer.zero_grad() # used to clear the gradients of all model parameters before the next iteration.\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient descent or adam step-> used to update the model's parameters based on the computed gradients.\n",
    "            optimizer.step()\n",
    "        mean_loss = sum(losses)/len(losses)\n",
    "        print(f\"loss at each epoch {mean_loss:.5f}\")\n",
    "\n",
    "train_nn(model, num_epochs, loader=train_loader, loss_fn=loss_fn, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Model Evaluation Section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Loop through the data\n",
    "        for x, y in loader:\n",
    "\n",
    "            # Move data to device\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            x= x.reshape(x.shape[0], -1)\n",
    "            # Forward pass\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "\n",
    "            # Check how many we got correct\n",
    "            num_correct += (predictions == y).sum()\n",
    "\n",
    "            # Keep track of number of samples\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    model.train()\n",
    "    return num_correct / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 94.55\n",
      "Accuracy on test set: 94.41\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy on training & test to see how good our model\n",
    "print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")\n",
    "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# Create a tensor\n",
    "x = torch.arange(12)\n",
    "# Reshape the tensor\n",
    "y = x.view(3, 4)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.7752, 0.6201],\n",
       "          [0.5137, 0.0647],\n",
       "          [0.1324, 0.3990],\n",
       "          [0.7703, 0.9250]],\n",
       " \n",
       "         [[0.5311, 0.7856],\n",
       "          [0.4865, 0.6590],\n",
       "          [0.3296, 0.0250],\n",
       "          [0.2081, 0.0032]],\n",
       " \n",
       "         [[0.8647, 0.2242],\n",
       "          [0.7728, 0.1922],\n",
       "          [0.5120, 0.8592],\n",
       "          [0.5363, 0.8615]]]),\n",
       " torch.Size([2, 3, 4]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "x = torch.rand(2, 3, 4)\n",
    "\n",
    "# Permute dimensions\n",
    "y = x.permute(1, 2, 0)\n",
    "y, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

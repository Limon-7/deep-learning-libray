{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***Epoch:***\n",
    "An epoch completes once a whole dataset has undergone forward propagation and backpropagation. Passing the whole dataset at once causes the underfitting of the curve. Therefore, we must give the entire dataset through the neural network model more than once to make the fitting curve from underfitting to optimal. But it can also cause overfitting of the curve if there are more epochs than needed.\n",
    "\n",
    "##### ***Batch Size:***\n",
    "The total number of data points in a single batch passed through the neural networks is called batch size.\n",
    "\n",
    "Sometimes the whole dataset can not be passed through the neural network at once due to insufficient memory or the dataset being too large. We divide the entire dataset into smaller numbers of parts called batches. These batches are passed through the model for training.\n",
    "\n",
    "#### ***Iteration:***\n",
    "The total ***number of batches*** needed to complete one epoch is called iteration.\n",
    "For example:` #no of dataset = 1000 #batch_size = 100 total #no of batch=1000/100= 10`. So `10` iteration is needed to complete ***one epoch.***\n",
    "\n",
    "#### ***Dataset:***\n",
    "Dataset stores the samples and their corresponding labels. `torch.utils.data.Dataset`\n",
    "\n",
    "#### ***DataLoader:***\n",
    "DataLoader wraps an iterable around the Dataset to enable easy access to the samples. `torch.utils.data.DataLoader`. To use the Dataloader, we need to set the following parameters:\n",
    "\n",
    "1. data the training data that will be used to train the model; and test data to evaluate the model\n",
    "2. batch size the number of records to be processed in each batch\n",
    "3. shuffle the randoms sample of the data by indices.\n",
    "4. Syntax: `train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)`\n",
    "\n",
    "[More Info](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "\n",
    "#### ***ToTensor:***\n",
    "`ToTensor` converts a PIL image or NumPy ndarray into a FloatTensor and scales the image's pixel intensity values in the range [0., 1.]\n",
    "\n",
    "\n",
    "#### ***collate_fn:*** \n",
    "The collate_fn parameter is a function that defines how individual data samples from your dataset should be batched together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WineDataSet(Dataset):\n",
    "    def __init__(self):\n",
    "        # Initialize data, download, etc.\n",
    "        # read with numpy or pandas\n",
    "        xy= np.loadtxt(\"../data/wine/wine.csv\", delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.n_samples=xy.shape[0]\n",
    "        self.x=torch.from_numpy(xy[:,1:]) # This will create a feature vectors of 1 * 13\n",
    "        self.y=torch.from_numpy(xy[:,[0]]) # This will create a level\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  tensor([1.3200e+01, 1.7800e+00, 2.1400e+00, 1.1200e+01, 1.0000e+02, 2.6500e+00,\n",
      "        2.7600e+00, 2.6000e-01, 1.2800e+00, 4.3800e+00, 1.0500e+00, 3.4000e+00,\n",
      "        1.0500e+03]) \n",
      " Lebels:  tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "dataset = WineDataSet()\n",
    "first_data = dataset[1]\n",
    "feature, label = first_data\n",
    "print(\"Features: \",feature, \"\\n Lebels: \", label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Each iteration below returns a batch of features and labels(containing batch_size=4 features and labels respectively).` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1870e+01, 4.3100e+00, 2.3900e+00, 2.1000e+01, 8.2000e+01, 2.8600e+00,\n",
      "         3.0300e+00, 2.1000e-01, 2.9100e+00, 2.8000e+00, 7.5000e-01, 3.6400e+00,\n",
      "         3.8000e+02],\n",
      "        [1.2080e+01, 1.8300e+00, 2.3200e+00, 1.8500e+01, 8.1000e+01, 1.6000e+00,\n",
      "         1.5000e+00, 5.2000e-01, 1.6400e+00, 2.4000e+00, 1.0800e+00, 2.2700e+00,\n",
      "         4.8000e+02],\n",
      "        [1.2850e+01, 1.6000e+00, 2.5200e+00, 1.7800e+01, 9.5000e+01, 2.4800e+00,\n",
      "         2.3700e+00, 2.6000e-01, 1.4600e+00, 3.9300e+00, 1.0900e+00, 3.6300e+00,\n",
      "         1.0150e+03],\n",
      "        [1.3940e+01, 1.7300e+00, 2.2700e+00, 1.7400e+01, 1.0800e+02, 2.8800e+00,\n",
      "         3.5400e+00, 3.2000e-01, 2.0800e+00, 8.9000e+00, 1.1200e+00, 3.1000e+00,\n",
      "         1.2600e+03]]) tensor([[2.],\n",
      "        [2.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "## DataLoader\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=4,\n",
    "                          shuffle=True)\n",
    "\n",
    "# convert to an iterator and look at one random sample\n",
    "dataiter = iter(train_loader)\n",
    "data = next(dataiter)\n",
    "features, labels = data\n",
    "print(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n",
      "Epoch: 1/2, Step 5/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 10/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 15/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 20/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 25/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 30/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 35/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 40/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 1/2, Step 45/45| Inputs torch.Size([2, 13]) | Labels torch.Size([2, 1])\n",
      "Epoch: 2/2, Step 5/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 10/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 15/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 20/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 25/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 30/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 35/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 40/45| Inputs torch.Size([4, 13]) | Labels torch.Size([4, 1])\n",
      "Epoch: 2/2, Step 45/45| Inputs torch.Size([2, 13]) | Labels torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# Dummy Training loop\n",
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i,(inputs, labels) in enumerate(train_loader):\n",
    "        # here: 178 samples, batch_size = 4, n_iters=178/4=44.5 -> 45 iterations\n",
    "        # Run your training process\n",
    "        if (i+1) % 5 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Step {i+1}/{n_iterations}| Inputs {inputs.shape} | Labels {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12000, 1, 28, 28])\n",
      "torch.Size([12000])\n",
      "torch.Size([12000, 1, 28, 28])\n",
      "torch.Size([12000])\n",
      "torch.Size([12000, 1, 28, 28])\n",
      "torch.Size([12000])\n",
      "torch.Size([12000, 1, 28, 28])\n",
      "torch.Size([12000])\n",
      "torch.Size([12000, 1, 28, 28])\n",
      "torch.Size([12000])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=True, \n",
    "                                           transform=torchvision.transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', \n",
    "                                           train=False, \n",
    "                                           transform=torchvision.transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=12000, \n",
    "                                           shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=3, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# look at one random sample\n",
    "# dataiter = iter(test_loader)\n",
    "# data = next(dataiter)\n",
    "# inputs, targets = data\n",
    "# print(inputs.shape, targets.shape)# batch size, color channel, height, width\n",
    "for images, labels in train_loader:\n",
    "    print(images.size())  # Output: torch.Size([64, 1, 28, 28]) for MNIST\n",
    "    print(labels.size())  # Output: torch.Size([64])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000 0 59999\n",
      "12000 59991\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valid_size = 0.2\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "print(num_train, indices)\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "print(len(train_idx),min(train_idx), max(train_idx))\n",
    "print(len(valid_idx), max(valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size):\n",
    "\n",
    "    # percentage of training set to use as validation\n",
    "    valid_size = 0.2\n",
    "\n",
    "    # convert data to torch.FloatTensor\n",
    "    transform = transforms.ToTensor()\n",
    "\n",
    "    # choose the training and test datasets\n",
    "    train_data = datasets.MNIST(root='./data', \n",
    "                                train=True,\n",
    "                                download=True, \n",
    "                                transform=transform)\n",
    "\n",
    "    test_data = datasets.MNIST(root='./data',\n",
    "                               train=False,\n",
    "                               download=True,\n",
    "                               transform=transform)\n",
    "\n",
    "    # obtain training indices that will be used for validation\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "    \n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "    \n",
    "    # load training data in batches\n",
    "    train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=0)\n",
    "    \n",
    "    # load validation data in batches\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                               batch_size=batch_size,\n",
    "                                               sampler=valid_sampler,\n",
    "                                               num_workers=0)\n",
    "    \n",
    "    # load test data in batches\n",
    "    test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                              batch_size=batch_size,\n",
    "                                              num_workers=0)\n",
    "    \n",
    "    return train_loader, test_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
